---
title: "Homework4"
author: "KiseokUchicago"
date: "2020-11-01"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path='Figs/',
                      error=TRUE, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE)
```

## Homework4
## Written assignment for Stat 30750
Professor: **Fatma Terzioglu**  
Student: **Kiseok Lee**  

### (2)
(Practical efficiency of LU decomposition) Generate an n x n matrix A with entries drawn independently from the standard normal distribution N(0; 1). Our goal in this exercise is to solve Ax = b for K different b's, where b is an n-vector with entries drawn independently from N(0, 1). Keep track of the computation time for the two methods below.

### (2)-(a)
For n=100, 200, 300, 400, solve Ax=b for x, for K different b, where K = 300, 500.

```{r 'a'}
library(tictoc)
library(dplyr)

# data collecting dataframe
df_time <- data.frame(n =double(), k=double())
df_time

tic.clearlog()
for (n in c(100,200,300,400)){
  print(n)
  A <- matrix(nrow=n, ncol=n)
  for (i in 1:dim(A)[1]){
      A[i,]=rnorm(n)
  }
  dim(A)
  # Create k different b from standard normal distribution N(0,1)
  for (k in c(300,500)){  # k = 300,500
      B <- matrix(nrow=n, ncol=k)
      # I created an matrix containing of all K different b, so that the time of the random sampling is not included the computation time.
      for (j in 1:dim(B)[2]){
          B[,j]=rnorm(n)
      }
      # Solve x for all K different b
      tic()
      for (colm in (1:k)){
        x <- solve(A,B[,colm])
      }
      toc(log=T)
      df_time=rbind(df_time, c(n,k))
    }
}

log.lst <- tic.log(format = FALSE)
tic.clearlog()

timings <- unlist(lapply(log.lst, function(x) x$toc - x$tic))
df_time$comp_time <- timings
colnames(df_time) <- c('n','K','comp_time')
df_time

```

### (2)-(b)
For n = 100; 200; 300; 400, first obtain A = LU. Then For K different b, solve Lc = b for c
followed by solving Ux = c for x. Do so for K = 300; 500.

Due to limit of my computing power, I will use K=3,5. It takes more than 3hr to do for K=300, 500, according to my estimation.

```{r 'b'}
# install.packages('matrixcalc')
library(matrixcalc)

# data collecting dataframe
df_time2 <- data.frame(n =double(), k=double())
df_time2

tic.clearlog()
for (n in c(100,200,300,400)){
  print(n)
  A <- matrix(nrow=n, ncol=n)
  for (i in 1:dim(A)[1]){
      A[i,]=rnorm(n)
  }
  dim(A)
  # Create k different b from standard normal distribution N(0,1)
  for (k in c(3,5)){  # k = 300,500
      B <- matrix(nrow=n, ncol=k)
      # I created an matrix containing of all K different b, so that the time of the random sampling is not included the computation time.
      for (j in 1:dim(B)[2]){
          B[,j]=rnorm(n)
      }
      # Solve x for all K different b
      tic()
      for (colm in (1:k)){
        luA <- lu.decomposition( A )
        L <- luA$L
        U <- luA$U
        c <- solve(L,B[,colm])
        x <- solve(U,c)
      }
      toc(log=T)
      df_time2=rbind(df_time2, c(n,k))
    }
}

log.lst <- tic.log(format = FALSE)
tic.clearlog()

timings <- unlist(lapply(log.lst, function(x) x$toc - x$tic))
df_time2$comp_time <- timings
colnames(df_time2) <- c('n','K','comp_time')
df_time2

# estimate K=300, 500 by multiplying by 100 to the time K=3, K=5
df_time2$K <- df_time2$K * 100
df_time2$comp_time <- df_time$comp_time * 100
df_time2

```

### (2)-(c)
Plot the computation times as a line graph for each K, with n as the horizontal axis, computa
tion time (or log of computation time) as the vertical axis. Plot the two line graphs of K = 300 and 500 on the same plot. Compare and comment.


```{r 'c'}
library(ggplot2)
theme_set(theme_bw())

# merge 2 dataframe of df_time and df_time2
df_time$K <- paste0('Ax=b_',df_time$K)
df_time2$K <- paste0('LUx=b_',df_time2$K)
df_time3 <- bind_rows(df_time, df_time2)

# plot for Ax=b method
df_time3$K <- factor(df_time3$K)
p1 <- ggplot(df_time3,aes(x = n,y = log(comp_time))) +
  geom_line(aes(color=K)) +
  scale_color_brewer(palette='Set2') +
  scale_y_continuous(breaks=seq(-3,8,1))+
  xlab("\n Dimension of matrix(n)") +
  ylab("Computational time\n (log (seconds)) ") +
  ggtitle("Ax=b vs LUx=b method")+
  ## adjust positions
  theme(plot.title = element_text(size = 20,hjust = 0.5, family="serif")) + 
  theme(axis.title.x = element_text(size = 15,hjust = 0.5, family="serif")) + 
  theme(axis.title.y = element_text(size = 15,hjust = 0.5, family="serif")) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust=0.3,size=11))+
  theme(axis.text.y = element_text(size=10))+
  theme(panel.grid.major = element_blank()) +
  theme(panel.grid.minor = element_blank())
p1

```
When you compare the method (a) Ax=b and (b) LUx=b, we can see from the graph that Ax=b method is more than 10,000~100,000 times faster than LU factorization method in all n(100, 200, 300, 400). Therefore, LU decomposition method is practically inefficient compared to the basic method solve() in R. However, there is a possibility that solve() method might be using a more efficient factorization or decomposition method than LU factorization. Also method (b) needs to solve two equations: Lc=b and Ux=c, which could have contributed to longer length of time. K=300 and K=500 does not differ much (less than 10x) in the computational time.

### (3)
(Condition number/Sensitivity of linear systems) 
Consider a linear system Ax = b, where A is invertible and b is nonzero. Let delta x and delta b denote errors or perturbations in x and b: A(x+delta x) = b+delta b.

### (3)-(a)
Derived by hand in the written assingment.

### (3)-(b)
(i)
```{r}
A = matrix( c(10000,9999,9999,9998), nrow=2, ncol=2)
A
b <- c(19999, 19997)
x <- solve(A,b)
x

print(paste0('Solution x is ',x))

```

(ii)
```{r}
b_hat <- c(19998.999, 19997.001)
x_hat <- solve(A,b_hat)
x_hat

euc_norm <- function(x) sqrt(sum(x^2))
error <- euc_norm(x_hat-x)/euc_norm(x)
error

print(paste0('Relative error is ',error))

```

(iii)
```{r}
b_hat2 <- c(19998.99, 19997.01)
x_hat2 <- solve(A,b_hat2)
x_hat2

euc_norm <- function(x) sqrt(sum(x^2))
error <- euc_norm(x_hat2-x)/euc_norm(x)
error

print(paste0('Relative error is ',error))

```

(iv)
```{r}
# install.packages('matlib')
library(matlib)

# condition number with Euclidean norm
kappa_euc <- euc_norm(A %*% x) * euc_norm(inv(A) %*% x)

# condition number using Frobenius form
kappa_fro <- sqrt(sum(A ** 2)) * sqrt(sum(inv(A) ** 2))
kappa_fro

print(paste0('Kappa value is ',kappa_fro))

```

Kappa value from Frobenius form with matrix A is almost 4x10^8. In part (a), the Kappa value was n (if the matrix is 2x2 matrix, then 2). Therefore, compared to the kappa value of matrix in part (b) is 2x10^8 times bigger than that of diagonal matrix in part (a). The larger the kappa value, the more unstable the linear system. Thus, it could be concluded that diagonal matrix is much more stable than the matrix with huge values in a ij (i != j) indices.

### (3)-(c)
Refer to the exercise in the HW3 30750 supplement, on Hilbert matrices, which occur naturally in approximations of functions by polynomials. Compute kappa F (A) of the n x n Hilbert matrix for n = 4 and n = 20 (you may use a built-in operation, but if so, try to write your own code to do the computation by hand as well { include your work on this in your submission). Use Matlab, Octave, R, or Julia for the computation. Comment on the sensitivity of the matrices in comparison to each other. (Note: You may use n = 4 and n = 12 if singularity causes no-output for larger n.)

```{r}
library(fractional)

for (n in c(4,20)){
  H <- matrix(nrow=n, ncol=n) %>% fractional
  for (i in 1:dim(H)[1]){
    for (j in 1:dim(H)[2]){
      H[i,j]=(i+j-1)^(-1)
    }
  }
  print(H)
  H_inverse = solve(H, tol = NULL)
  kappa_fro <- sqrt(sum(H ** 2)) * sqrt(sum(H_inverse ** 2))
  print(paste0('For Hilbert matrix of ',n,'x',n))
  print(paste0('kappa F(H)=',kappa_fro))
}

```
Sensitivity grows exponentially as the Hilbert matrix have bigger dimensions.
kappa F(H)=1962058291245698816, when 20x20 matrix. 
kappa F(H)=15613.7935596424, when 4x4 matrix. 
This is a difference of factor of 1.256682e+14. Therefore, we could conclude that matrix with smaller dimensions is much more stable to perturbations.

